{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project: Data_Science_Receipt_Identification'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Project: Data_Science_Receipt_Identification\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Read data, Generate samples'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Read data, Generate samples'''\n",
    "samples = list(zip(images, labels))\t# zip two lists for convenience\n",
    "\n",
    "print(len(samples))\n",
    "print('Number of images used:',len(samples))\n",
    "plt.imshow(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Generator'''\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "print(len(train_samples), len(validation_samples))\n",
    "\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "ch, row, col = 3, 240, 320  #\n",
    "ch, row, col = 3, 120, 160  #\n",
    "ch, row, col = 3, 66, 220\n",
    "# ch, row, col = 3, 340, 640 # not resize\n",
    "# ch, row, col = 3 170, 320 #  resize to half\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        # shuffle the data\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            speeds = []\n",
    "            for batch_sample in batch_samples:\n",
    "                # read image and append images\n",
    "#                 image_filename = batch_sample[0]\n",
    "#                 image = cv2.imread(image_filename)\n",
    "#                 image = cv2.resize(image, (col, row), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                image = batch_sample[0]\n",
    "    \n",
    "                images.append(image)\n",
    "\n",
    "                speed = float(batch_sample[1])\n",
    "                speeds.append(speed)\n",
    "\n",
    "                # # flip image to generalize\n",
    "                # image_flipped = np.fliplr(image)\n",
    "                # images.append(image)\n",
    "                # speeds.append(speed)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(speeds)\n",
    "\n",
    "\n",
    "            # print(X_train[0].shape)\n",
    "            # print(len(X_train), len(y_train))\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ec18ba24225a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_Nvidia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'row' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Nvidia using old version of keras\n",
    "'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "input_shape = (row, col, ch)\n",
    "def train_Nvidia(input_shape):\n",
    "    \"\"\"\n",
    "    Train the model using nvidia architecture\n",
    "    \"\"\"\n",
    "    print(\"train using nvidia model\")\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x:x / 127.5 - 0.5, input_shape=input_shape))\n",
    "\n",
    "    model.add(Convolution2D(24, 5, 5, border_mode='valid', subsample=(2,2), activation=\"relu\"))\n",
    "#     model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Convolution2D(36, 5, 5, border_mode=\"valid\", subsample=(2,2), activation=\"elu\"))\n",
    "#     model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Convolution2D(48, 5, 5, border_mode=\"valid\", subsample=(2,2), activation=\"elu\"))\n",
    "#     model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode=\"valid\", activation=\"elu\"))\n",
    "#     model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode=\"valid\", activation=\"elu\"))\n",
    "#     model.add(BatchNormalization(axis=-1))\n",
    "\n",
    "    model.add(Flatten())\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(Dense(100, activation=\"elu\"))\n",
    "    model.add(Dense(50, activation=\"elu\"))\n",
    "    model.add(Dense(10, activation=\"elu\"))\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    \n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=adam)\n",
    "\n",
    "    \n",
    "    filepath = 'nvidia-model.h5'\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                                  patience=1, \n",
    "                                  verbose=1, \n",
    "                                  min_delta = 0.23,\n",
    "                                  mode='min',)\n",
    "    modelCheckpoint = ModelCheckpoint(filepath, \n",
    "                                      monitor = 'val_loss', \n",
    "                                      save_best_only = True, \n",
    "                                      mode = 'min', \n",
    "                                      verbose = 1\n",
    "                                     )\n",
    "#                                      save_weights_only = False)\n",
    "    callbacks_list = [modelCheckpoint, earlyStopping]\n",
    "\n",
    "    # history = model.fit_generator(\n",
    "    #         train_generator, \n",
    "    #         steps_per_epoch = 400, \n",
    "    #         epochs = 25,\n",
    "    #         callbacks = callbacks_list,\n",
    "    #         verbose = 1,\n",
    "    #         validation_data = valid_generator,\n",
    "    #         validation_steps = val_size)\n",
    "    history_object = model.fit_generator(\n",
    "                        train_generator, \n",
    "#                         steps_per_epoch = 400, \n",
    "\n",
    "                        samples_per_epoch=len(train_samples), \n",
    "                        validation_data=validation_generator,\n",
    "                        nb_val_samples=len(validation_samples), \n",
    "                        nb_epoch=25,\n",
    "                        callbacks = callbacks_list)\n",
    "    model.summary()\n",
    "\n",
    "    # save the model\n",
    "#     os.chdir('.')\n",
    "#     model.save('model_opticalFlow_66_220.h5')\n",
    "    print ('saved')\n",
    "    return history_object\n",
    "history_object = train_Nvidia(input_shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''visualization'''\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
