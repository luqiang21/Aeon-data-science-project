{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project: Data_Science_Receipt_Identification'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Project: Data_Science_Receipt_Identification\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4502 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4503 ['EXT_ID', 'STORENAME', 'IsWalmart']\n",
      "58c47902e4b06d16e3cd3541.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 976/4502 [05:40<19:04,  3.08it/s]"
     ]
    }
   ],
   "source": [
    "file = 'training_data.csv'\n",
    "reader = csv.reader(open(file))\n",
    "samples = []\n",
    "for line in reader:\n",
    "    samples.append(line)\n",
    "print(len(samples), samples[0])\n",
    "samples = samples[1:]\n",
    "print(samples[0][0] + '.jpg')\n",
    "\n",
    "ch, row, col = 3, 66, 220\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for sample in tqdm(samples):\n",
    "    image_path = 'images/' + sample[0] + '.jpg'\n",
    "    image = cv2.imread(image_path)\n",
    "    resized_image = cv2.resize(image, (col, row), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    images.append(resized_image)\n",
    "\n",
    "    label = sample[-1]\n",
    "    labels.append(label)\n",
    "print('images and labels read.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''One hot encoding'''\n",
    "# file = 'training_data.csv'\n",
    "# reader = csv.reader(open(file))\n",
    "# samples = []\n",
    "# for line in reader:\n",
    "#     samples.append(line)\n",
    "# samples = samples[1:]\n",
    "# labels = []\n",
    "# for sample in samples:\n",
    "#     labels.append(sample[-1])\n",
    "    \n",
    "from keras.utils.np_utils import to_categorical\n",
    "labels = labels[1:]\n",
    "images = images[1:]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "encoded_labels = encoder.transform(labels)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "one_hot_labels = np_utils.to_categorical(encoded_labels)\n",
    "print(one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Image Preprocessing, resize to 66x220, BGR to Gray'''\n",
    "preprocessed_images = []\n",
    "for i in tqdm(range(len(images))):\n",
    "    image = images[i]\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    preprocessed_images.append(gray)\n",
    "\n",
    "n = 1000\n",
    "plt.imshow(images[n])    \n",
    "plt.imshow(preprocessed_images[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Read data, Generate samples'''\n",
    "samples = list(zip(preprocessed_images, one_hot_labels)) # zip two lists for convenience\n",
    "\n",
    "print(len(samples))\n",
    "print('Number of images used:',len(samples))\n",
    "plt.imshow(samples[0][0])\n",
    "print(samples[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(samples))\n",
    "print(len(one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Generator'''\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "print(len(train_samples), len(validation_samples))\n",
    "\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "ch, row, col = 3, 240, 320  #\n",
    "ch, row, col = 3, 120, 160  #\n",
    "ch, row, col = 3, 66, 220\n",
    "# ch, row, col = 3, 340, 640 # not resize\n",
    "# ch, row, col = 3 170, 320 #  resize to half\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        # shuffle the data\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            isWalmarts = []\n",
    "            for batch_sample in batch_samples:\n",
    "                # read image and append images\n",
    "#                 image_filename = batch_sample[0]\n",
    "#                 image = cv2.imread(image_filename)\n",
    "#                 image = cv2.resize(image, (col, row), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                image = batch_sample[0]\n",
    "    \n",
    "                images.append(image)\n",
    "\n",
    "                isWalmart = batch_sample[1]\n",
    "                isWalmarts.append(isWalmart)    \n",
    "                \n",
    "                # # flip image to generalize\n",
    "                # image_flipped = np.fliplr(image)\n",
    "                # images.append(image)\n",
    "                # speeds.append(speed)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(isWalmarts)\n",
    "\n",
    "\n",
    "            # print(X_train[0].shape)\n",
    "            # print(len(X_train), len(y_train))\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Nvidia using old version of keras\n",
    "'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "ch, row, col = 3, 66, 220\n",
    "\n",
    "input_shape = (row, col, ch)\n",
    "def train_Nvidia(input_shape):\n",
    "    \"\"\"\n",
    "    Train the model using nvidia architecture\n",
    "    \"\"\"\n",
    "    print(\"train using nvidia model\")\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x:x / 127.5 - 0.5, input_shape=input_shape))\n",
    "\n",
    "    model.add(Convolution2D(24, 5, 5, border_mode='valid', subsample=(2,2), activation=\"relu\"))\n",
    "#     model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Convolution2D(36, 5, 5, border_mode=\"valid\", subsample=(2,2), activation=\"elu\"))\n",
    "#     model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Convolution2D(48, 5, 5, border_mode=\"valid\", subsample=(2,2), activation=\"elu\"))\n",
    "#     model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode=\"valid\", activation=\"elu\"))\n",
    "#     model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode=\"valid\", activation=\"elu\"))\n",
    "#     model.add(BatchNormalization(axis=-1))\n",
    "\n",
    "    model.add(Flatten())\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(Dense(100, activation=\"elu\"))\n",
    "    model.add(Dense(50, activation=\"elu\"))\n",
    "    model.add(Dense(10, activation=\"elu\"))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(1, activation=\"linear\"))\n",
    "#     model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "#     model.compile(loss='mse', optimizer=adam)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "\n",
    "    \n",
    "    filepath = 'nvidia-model.h5'\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                                  patience=1, \n",
    "                                  verbose=1, \n",
    "                                  min_delta = 0.23,\n",
    "                                  mode='min',)\n",
    "    modelCheckpoint = ModelCheckpoint(filepath, \n",
    "                                      monitor = 'val_loss', \n",
    "                                      save_best_only = True, \n",
    "                                      mode = 'min', \n",
    "                                      verbose = 1\n",
    "                                     )\n",
    "#                                      save_weights_only = False)\n",
    "    callbacks_list = [modelCheckpoint, earlyStopping]\n",
    "\n",
    "    # history = model.fit_generator(\n",
    "    #         train_generator, \n",
    "    #         steps_per_epoch = 400, \n",
    "    #         epochs = 25,\n",
    "    #         callbacks = callbacks_list,\n",
    "    #         verbose = 1,\n",
    "    #         validation_data = valid_generator,\n",
    "    #         validation_steps = val_size)\n",
    "    history_object = model.fit_generator(\n",
    "                        train_generator, \n",
    "#                         steps_per_epoch = 400, \n",
    "\n",
    "                        samples_per_epoch=len(train_samples), \n",
    "                        validation_data=validation_generator,\n",
    "                        nb_val_samples=len(validation_samples), \n",
    "                        nb_epoch=25,\n",
    "                        callbacks = callbacks_list)\n",
    "    model.summary()\n",
    "\n",
    "    # save the model\n",
    "#     os.chdir('.')\n",
    "#     model.save('model_opticalFlow_66_220.h5')\n",
    "    print ('saved')\n",
    "    return history_object, model\n",
    "history_object, model = train_Nvidia(input_shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''visualization'''\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "prediction\n",
    "'''\n",
    "from keras.models import load_model\n",
    "ch, row, col = 3, 66, 220\n",
    "\n",
    "model_name = 'nvidia-model.h5'\n",
    "model = load_model(model_name)\n",
    "image_name = 'images/58a0deb9e4b0314ab22f9906.jpg'\n",
    "# image_name = 'images/58b61e0ee4b0d23e37734d43.jpg'\n",
    "image = cv2.imread(image_name)\n",
    "image = cv2.resize(image, (col, row), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "image_array = np.asarray(image)\n",
    "# resize\n",
    "#         image_array = cv2.resize(image_array, (col, row), interpolation=cv2.INTER_AREA)\n",
    "# predict\n",
    "print(model.predict_classes(image_array[None, :, :, :])[0])\n",
    "# prediction = float(model.predict(image_array[None, :, :, :], batch_size=1))\n",
    "# print(prediction)\n",
    "print(np.max(model.predict_proba(image_array[None, :, :, :])))\n",
    "isWalmart = model.predict_classes(image_array[None, :, :, :])[0]\n",
    "prob = np.max(model.predict_proba(image_array[None, :, :, :]))\n",
    "print(isWalmart, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = 'test_data.csv'\n",
    "reader = csv.reader(open(file))\n",
    "samples = []\n",
    "for line in reader:\n",
    "    samples.append(line)\n",
    "print(len(samples), tokens[0])\n",
    "samples = samples[1:]\n",
    "print(samples[0][0] + '.jpg')\n",
    "\n",
    "ch, row, col = 3, 66, 220\n",
    "\n",
    "test_images_dic = {}\n",
    "labels = []\n",
    "for sample in tqdm(samples):\n",
    "    image_path = 'images/' + sample[0] + '.jpg'\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (col, row), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    test_images_dic[sample[0]] = image\n",
    "\n",
    "print('test images read.') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for image_name, test_image in test_images_dic.items():\n",
    "    prediction = model.predict_classes(test_image[None, :, :, :])[0]\n",
    "    if prediction == 1:\n",
    "        isWalmart = 'TRUE'\n",
    "    else:\n",
    "        isWalmart = 'FALSE'\n",
    "    prob = np.max(model.predict_proba(test_image[None, :, :, :]))\n",
    "    data.append(image_name + ',' + isWalmart + ',' + str(prob) + '\\n')\n",
    "    \n",
    "with open(\"results.csv\", \"w\") as csv_file:\n",
    "#         writer = csv.writer(csv_file, delimiter=',')\n",
    "    csv_file.write('EXT_ID,WalmartReceipt,PredictionScore\\n')\n",
    "\n",
    "    for line in data:\n",
    "        csv_file.write(line)\n",
    "            \n",
    "print('results are written to csv.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
